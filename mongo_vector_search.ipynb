{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_INDEX_NAME = \"title_body_vector_index\"\n",
    "INDEX_FIELD = \"title_body_embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('667d1ef6fc45eb48396a6a06'), 'date': datetime.datetime(2024, 6, 25, 21, 49), 'title': \"Boston scientists create AI model to 'catch Alzheimer's disease early'\", 'body': \"Researchers say they've created a promising AI model that predicts the likelihood of someone developing Alzheimer's early. The Boston University researchers on Tuesday announced that they designed the new artificial intelligence computer program â€” which identifies those with mild cognitive impairment who are likely to develop Alzheimer's within six years.\", 'url': 'https://www.msn.com/en-us/health/other/boston-scientists-create-ai-model-to-catch-alzheimer-s-disease-early/ar-BB1oT292', 'image': 'https://img-s-msn-com.akamaized.net/tenant/amp/entityid/BB1oSZDb.img?w=4000&h=2609&m=4&q=79', 'source': 'Tribune News Service on MSN.com', 'found_at': datetime.datetime(2024, 6, 26, 9, 51, 56, 553000)}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pymongo\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# connect to your Atlas cluster\n",
    "client = pymongo.MongoClient(os.getenv(\"MONGODB_URI\"))\n",
    "\n",
    "db = client[\"blogdb\"]\n",
    "\n",
    "collection = db[\"ai_news\"]\n",
    "\n",
    "print(collection.find_one())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add vectors to existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"text-embedding-3-small\"\n",
    "DIMENSIONS = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "Embedding = list[float]\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def get_embeddings_batch(texts: list[str], dimensions: int) -> list[Embedding]:\n",
    "    assert len(texts) <= 2048, \"The number of texts should be less than 2048\"\n",
    "\n",
    "    response = client.embeddings.create(input=texts, model=MODEL, dimensions=dimensions)\n",
    "\n",
    "    return [x.embedding for x in response.data]\n",
    "\n",
    "def get_embedding(text: str, dimensions: int) -> Embedding:\n",
    "    return get_embeddings_batch([text], dimensions)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/13901 documents\n",
      "Processed 2000/13901 documents\n",
      "Processed 3000/13901 documents\n",
      "Processed 4000/13901 documents\n",
      "Processed 5000/13901 documents\n",
      "Processed 6000/13901 documents\n",
      "Processed 7000/13901 documents\n",
      "Processed 8000/13901 documents\n",
      "Processed 9000/13901 documents\n",
      "Processed 10000/13901 documents\n",
      "Processed 11000/13901 documents\n",
      "Processed 12000/13901 documents\n",
      "Processed 13000/13901 documents\n",
      "Processed 13901/13901 documents\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pymongo.operations import UpdateOne\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE: int = 1000\n",
    "\n",
    "\n",
    "def process_batch(batch: List[Dict[str, Any]]) -> None:\n",
    "    # Extract text from documents (combining title and body)\n",
    "    texts: List[str] = [f\"{doc['title']} - {doc['body']}\" for doc in batch]\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings: List[Embedding] = get_embeddings_batch(texts, dimensions=DIMENSIONS)\n",
    "\n",
    "    # Prepare bulk update operations\n",
    "    bulk_operations: List[UpdateOne] = []\n",
    "    for doc, embedding in zip(batch, embeddings):\n",
    "        bulk_operations.append(\n",
    "            UpdateOne(\n",
    "                {\"_id\": doc[\"_id\"]}, {\"$set\": {\"title_body_embedding\": embedding}}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Execute bulk update\n",
    "    if bulk_operations:\n",
    "        collection.bulk_write(bulk_operations)\n",
    "\n",
    "\n",
    "def update_embeddings(limit: int | None = None) -> None:\n",
    "    # Query for documents without title_body_embedding\n",
    "    query = {\n",
    "        \"$or\": [\n",
    "            {\"title_body_embedding\": {\"$exists\": False}},\n",
    "            {\"title_body_embedding\": None},\n",
    "            {\"title_body_embedding\": []},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Get total number of documents to update, respecting the limit\n",
    "    total_docs: int = (\n",
    "        min(collection.count_documents(query), limit)\n",
    "        if limit\n",
    "        else collection.count_documents(query)\n",
    "    )\n",
    "    processed_docs: int = 0\n",
    "\n",
    "    while processed_docs < total_docs:\n",
    "        # Calculate remaining documents to process\n",
    "        remaining_docs = total_docs - processed_docs\n",
    "        current_batch_size = min(BATCH_SIZE, remaining_docs)\n",
    "\n",
    "        # Fetch a batch of documents\n",
    "        cursor = collection.find(query, {\"title\": 1, \"body\": 1}).limit(\n",
    "            current_batch_size\n",
    "        )\n",
    "        batch: List[Dict[str, Any]] = list(cursor)\n",
    "\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        # Process the batch\n",
    "        process_batch(batch)\n",
    "\n",
    "        processed_docs += len(batch)\n",
    "        print(f\"Processed {processed_docs}/{total_docs} documents\")\n",
    "\n",
    "\n",
    "update_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AI Will Become Mathematicians' 'Co-Pilot'\n",
      "Date: 2024-06-08 00:00:00 \n",
      "URL: https://www.scientificamerican.com/article/ai-will-become-mathematicians-co-pilot/\n",
      "Score: 0.8364213705062866\n",
      "\n",
      "You even talked about getting that factor down to less than one. With AI, there's a real potential of doing that. I think in the future, instead of typing up our proofs, we would explain them to ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Title: AI isn't dumb, but it might be dumber than you think\n",
      "Date: 2024-06-25 16:30:00 \n",
      "URL: https://www.msn.com/en-au/news/techandscience/ai-isn-t-dumb-but-it-might-be-dumber-than-you-think/ar-BB1oS8xW\n",
      "Score: 0.831683874130249\n",
      "\n",
      "I recently couldn't remember the name of a pastry and tried describing it to three different AI chatbots. It didn't work. (The pastry was a frangipane tart.) Disappointments like those don't mean AI is useless.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Title: AI isn't dumb, but it might be dumber than you think\n",
      "Date: 2024-06-25 16:30:00 \n",
      "URL: https://www.msn.com/en-gb/news/techandscience/ai-isn-t-dumb-but-it-might-be-dumber-than-you-think/ar-BB1oS8xW\n",
      "Score: 0.8286367654800415\n",
      "\n",
      "The truth is that AI is fundamentally bad at many tasks. It requires you learn just the right words to coax the best out of it. Like all computers, AI will make different mistakes than people do, but it will make mistakes. And the AI that's foisted on you is sometimes just broken.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Title: AI Large Language Model Math Breakthroughs\n",
      "Date: 2024-06-15 23:59:00 \n",
      "URL: https://www.nextbigfuture.com/2024/06/ai-large-language-model-math-breakthroughs.html\n",
      "Score: 0.8151002526283264\n",
      "\n",
      "AI large language models have been especially weak on math. There are now several papers from Google Deep Mind, Alibaba and other universities where AI large language models are at Math Olympiad levels and multiple step reasoning even with small models.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Title: AI is in the tire-kicking phase\n",
      "Date: 2024-06-25 09:00:00 \n",
      "URL: https://www.infoworld.com/article/3715623/ai-is-in-the-tire-kicking-phase.html\n",
      "Score: 0.8077011704444885\n",
      "\n",
      "The clouds are mostly fattening their AI revenues through training models ... rather have an LLM hallucinate on an early version of marketing copy than your income statement. According to McKinsey, the top genAI performers tend to be those that \"have ...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"AI is good at math\"\n",
    "\n",
    "query_embedding = get_embedding(QUERY, dimensions=DIMENSIONS)\n",
    "\n",
    "LIMIT = 10\n",
    "NUM_CANDIDATES = 20 * LIMIT # should be between 10 and 20 times the limit\n",
    "\n",
    "# Sample vector search pipeline\n",
    "pipeline = [\n",
    "   {\n",
    "      \"$vectorSearch\": {\n",
    "            \"index\":VECTOR_INDEX_NAME,\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"path\": INDEX_FIELD,\n",
    "            \"numCandidates\": 100,\n",
    "            \"limit\": 5\n",
    "      }\n",
    "   },\n",
    "   {\n",
    "      \"$project\": {\n",
    "         \"_id\": 0,\n",
    "         \"title\": 1,\n",
    "         \"date\": 1,\n",
    "         \"url\": 1,\n",
    "         \"body\": 1,\n",
    "         \"score\": {\n",
    "            \"$meta\": \"vectorSearchScore\"\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "]\n",
    "\n",
    "results = collection.aggregate(pipeline)\n",
    "\n",
    "for i in results:\n",
    "   print(f\"Title: {i['title']}\\nDate: {i['date']} \\nURL: {i['url']}\\nScore: {i['score']}\\n\\n{i['body']}\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-db-crR3mef3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
